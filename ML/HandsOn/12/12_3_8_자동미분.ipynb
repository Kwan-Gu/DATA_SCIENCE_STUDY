{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "77ce815a0754f4627093f19bb851f9c1582d5575dae88618c8a1250e97309331"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 12.3.8. 자동 미분을 사용하여 그래디언트 계산하기\n",
    "## 자동 미분이란?\n",
    "- 신경망처럼 수만 개의 파라미터를 가진 복잡한 함수의 도함수(미분, 그래디언트)를 쉽게 계산할 수 있도록 해주는 도구\n",
    "- tensorflow.GradientTape() 을 활용\n",
    "- 예시: $w_1 = 5, w_2 = 3$ 일 때 도함수는 각각 36과 10임을 미분을 통해 구함\n",
    "\n",
    "$$f(w_1, w_2) = 3w_1^2 + 2w_1w_2$$\n",
    "\n",
    "$${df \\over {dw_1}} = 6w_1 + 2w_2$$\n",
    "\n",
    "$${df \\over {dw_2}} = 2w_1$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(36.0, shape=(), dtype=float32) tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.) # 두 변수 w1과 w2를 정의\n",
    "\n",
    "def f(w1, w2): # 함수 f 정의\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape: # tf.GradientTape 블록을 만들어, 관련된 연산을 기록\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1) # 그래디언트 확인\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "del tape # Tape 삭제하여 리소스 해제\n",
    "\n",
    "print(dz_dw1, dz_dw2)"
   ]
  },
  {
   "source": [
    "## 상수(constant)에 대한 자동 미분\n",
    "- 원칙적으로는 변수(Variable)에 대해서만 미분이 가능하기 때문에, 상수에 대한 미분은 None으로 계산"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.) # 두 상수 c1, c2를 정의\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape: # tf.GradientTape 블록을 만들어, 관련된 연산을 기록\n",
    "    z = f(c1, c2)\n",
    "\n",
    "dz_dc1 = tape.gradient(z, c1) # 그래디언트 확인\n",
    "dz_dc2 = tape.gradient(z, c2)\n",
    "del tape # Tape 삭제하여 리소스 해제\n",
    "\n",
    "print(dz_dc1, dz_dc2)"
   ]
  },
  {
   "source": [
    "- 변수, 상수에 관계없이 그래디언트를 구하고자 한다면, watch 메소드를 사용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(36.0, shape=(), dtype=float32) tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent = True) as tape: # tf.GradientTape 블록을 만들어, 관련된 연산을 기록\n",
    "    tape.watch(c1) # Variable, constant 상관없이 어떤 텐서라도 감시하여 관련된 연산을 기록하도록 설정\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "dz_dc1 = tape.gradient(z, c1) # 그래디언트 확인\n",
    "dz_dc2 = tape.gradient(z, c2)\n",
    "del tape # Tape 삭제하여 리소스 해제\n",
    "\n",
    "print(dz_dc1, dz_dc2)"
   ]
  },
  {
   "source": [
    "## 자동 미분 응용\n",
    "- 그래디언트를 계산할 때 일부를 제외하고 역전파되도록 설정하기 위해 tf.stop_gradient를 활용\n",
    "\n",
    "$$f(w_1, w_2) = 3w_1^2 + 2w_1w_2$$\n",
    "\n",
    "$${df \\over {dw_1}} = 6w_1$$\n",
    "\n",
    "$${df \\over {dw_2}} = None$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(30.0, shape=(), dtype=float32) None\n"
     ]
    }
   ],
   "source": [
    "def f(w1, w2): # 함수 f 정의\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2) # 그래디언트 일부분이 역전파되지 않도록 설정(tf.stop_gradient)\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape: # tf.GradientTape 블록을 만들어, 관련된 연산을 기록\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1) # 그래디언트 확인\n",
    "dz_dw2 = tape.gradient(z, w2)\n",
    "del tape # Tape 삭제하여 리소스 해제\n",
    "\n",
    "print(dz_dw1, dz_dw2)"
   ]
  },
  {
   "source": [
    "- 그래디언트를 해석적인 방법으로 구하기 위해서는 @tf.custom_gradient 데코레이터를 사용하고, 해석적으로 구한 도함수를 정의\n",
    "- 예시: softplus\n",
    "\n",
    "$$f(x) = log(exp(x) + 1)$$\n",
    "\n",
    "$${df \\over {dx}} = {1 \\over 1 + exp(-x)}$$\n",
    "\n",
    "<center><img src ='./figures/softplus.png'></center>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "x = tf.Variable([100.]) # 변수 x 정의\n",
    "\n",
    "def my_softplus(z): # 활성화함수 정의(위 그림 참고)\n",
    "    return tf.math.log(tf.exp(z) + 1.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x]) # 그래디언트가 nan으로 계산됨(수치적으로 불안정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "\n",
    "    def my_softplus_gradients(grad): # 해석적으로 구한 도함수 정의\n",
    "        return grad / (1. + 1. / exp)\n",
    "\n",
    "    return tf.math.log(exp + 1.), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x]) # 그래디언트 계산됨"
   ]
  },
  {
   "source": [
    "# 12.3.9. 사용자 정의 훈련 반복\n",
    "## 왜 필요한가?\n",
    "1. fit() 메소드 대신 원하는대로 훈련을 수행하기 위해\n",
    "2. 의도한대로 훈련이 잘 되고있는지 확신을 갖기 위해\n",
    "- 예시: 두 개의 다른 옵티마이저를 하나의 신경망에 사용(와이드 네트워크에는 Adam, 딥 네트워크에는 SGD)\n",
    "## 문제점\n",
    "- 버그가 발생하기 쉬움\n",
    "- 유지 보수가 어려움\n",
    "- 팀 작업에 어려움이 발생"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-310cd7b8def1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mn_epochs\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mn_steps\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0moptimizer\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNadam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mloss_fn\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 신경망 모델 생성\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'elu', kernel_initializer = 'he_normal', kernel_regularizer = l2_reg),\n",
    "    keras.layers.Dense(1 , kernel_regularizer = l2_reg)])\n",
    "\n",
    "# 훈련데이터 세트에서 배치를 랜덤하게 추출하는 함수 생성\n",
    "def random_batch(X, y, batch_size = 32):\n",
    "    idx = np.random.randint(len(X), size = batch_size) # 균일 분포의 정수 난수 생성[0 ~ len(X)-1]\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "# 훈련 상태를 출혁하는 함수 생성\n",
    "def print_status_bar(iteration, total, loss, metrics = None):\n",
    "    metrics = '-'.join([f'{m.name}: {m.result():.4f}' for m in [loss] + (metrics or [])])\n",
    "    end     = '' if iteration < total else '\\n'\n",
    "    print(f'\\r{iteration}/{total} - ' + metrics, end = end)\n",
    "\n",
    "# 하이퍼파라미터 정의\n",
    "n_epochs   = 5\n",
    "batch_size = 32\n",
    "n_steps    = len(X_train) // batch_size\n",
    "optimizer  = keras.optimizer.Nadam(lr = .01) \n",
    "loss_fn    = keras.losses.mean_squared_error\n",
    "mean_loss  = keras.metrics.Mean()\n",
    "metrics    = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# 사용자 정의 훈련 반복 수행\n",
    "for epoch in range(1, n_epochs + 1): # 훈련 에포크 반복\n",
    "    print(f'Epoch {epoch}/{n_epochs}')\n",
    "    for step in range(1, n_steps + 1): # 에포크 내 훈련 스텝 반복\n",
    "        X_batch, y_batch = random_batch(X_train, y_train) # 훈련데이터 세트에서 랜덤하게 배치 생성\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred    = model(X_batch, training = True) # 예측 생성\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) # loss_fn: 샘플마다 하나의 손실을 반환, reduce_mean을 통해 배치에 대한 평균 손실을 계산\n",
    "            loss      = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables) # 훈련 가능한 변수들에 대한 손실의 그래디언트를 계산\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # 경사하강법 수행\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics) # 각 스텝마다 훈련 상태 출력\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics) # 각 에포크마다 훈련 상태 출력\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states() # 평균 손실과 지표들을 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}