{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0888747915a6fa02208d3d726af7412a51f7d682d8658c5c9b27496b4f9df761d",
   "display_name": "Python 3.8.8 64-bit ('venv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "- 충분하지 않은 데이터로 이미지 분류기를 훈련하라면 사전훈련된 모델의 하위층을 사용하는 것이 좋음"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds "
   ]
  },
  {
   "source": [
    "### Xception 모델을 사용해 꽃 이미지를 분류하는 모델\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 적재\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True) #with_info=True : 데이터셋 정보를 줌\n",
    "dataset_size = info.splits[\"train\"].num_examples # 3670\n",
    "calss_names = info.features[\"label\"].names # [\"dendelion\",\"daisy\",...]\n",
    "n_classes = info.features[\"label\"].num_classes # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test set 나누기 \n",
    "\n",
    "# 처음 10%를 테스트셋, 다음 15% 검증셋, 남은 75% 훈련셋 \n",
    "# test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])\n",
    "# test_set = tfds.load(\"tf_flowers\", split = valid_split, as_supervised=True)\n",
    "# valid_set = tfds.load(\"tf_flowers\", split = valid_split, as_supervised=True)\n",
    "# train_set = tfds.load(\"tf_flowers\", split = train_split, as_supervised=True)\n",
    "\n",
    "\n",
    "test_set = tfds.load(\"tf_flowers\", split = 'train[:10%]', as_supervised=True)\n",
    "valid_set = tfds.load(\"tf_flowers\", split = 'train[10%:25%]', as_supervised=True)\n",
    "train_set = tfds.load(\"tf_flowers\", split = 'train[25%:]', as_supervised=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리\n",
    "\n",
    "# 224 X 224 크기로 조정 \n",
    "def preprocess(image,label):\n",
    "    resized_image = tf.image.resize(image,[224,224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 3개를 데이터셋에 적용\n",
    "# 배치크기를 지정 > 프리페치 적용 \n",
    "\n",
    "batch_size = 32\n",
    "train_size = train_set.shuffle(1000)\n",
    "train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증식을 추가하고 싶은 경우 : 훈련 이미지 랜덤하게 변경\n",
    "# 1) 이미지를 랜덤하게 자르기 : tf.image.random_crop()\n",
    "# 2) 이미지를 수평으로 랜덤하게 뒤집기 : tf.image.random_flip_left_right()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 16s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Xception 모델 로드\n",
    "\n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top = False)\n",
    "                #include_Top=False : 네트워크 최상층에 해당하는 GlobalAveragePooling layer와 dense layer 제외\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output) #새로운 GlobalAveragePooling layer 주가\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg) # 새로운 dense layer 추가\n",
    "model = keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 초기에는 사전훈련된 층의 가중치를 동결\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "86/86 [==============================] - 372s 4s/step - loss: 1.8903 - accuracy: 0.6843 - val_loss: 0.9163 - val_accuracy: 0.8621\n",
      "Epoch 2/5\n",
      "86/86 [==============================] - 361s 4s/step - loss: 0.5151 - accuracy: 0.9095 - val_loss: 0.7988 - val_accuracy: 0.8766\n",
      "Epoch 3/5\n",
      "86/86 [==============================] - 363s 4s/step - loss: 0.2725 - accuracy: 0.9352 - val_loss: 0.8340 - val_accuracy: 0.8639\n",
      "Epoch 4/5\n",
      "86/86 [==============================] - 409s 5s/step - loss: 0.1730 - accuracy: 0.9564 - val_loss: 0.6633 - val_accuracy: 0.8784\n",
      "Epoch 5/5\n",
      "86/86 [==============================] - 371s 4s/step - loss: 0.1113 - accuracy: 0.9656 - val_loss: 0.6618 - val_accuracy: 0.8693\n"
     ]
    }
   ],
   "source": [
    "#최적화 함수 지정\n",
    "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# fitting\n",
    "history = model.fit(train_set, epochs=5, validation_data=valid_set)\n",
    "\n",
    "# 75~80% 수준에 머뭄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'complie'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b340b5760c80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 최적화함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomplie\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'complie'"
     ]
    }
   ],
   "source": [
    "# 동결 해제\n",
    "for layer  in base_model.layers:\n",
    "    layer.trainable=True\n",
    "\n",
    "# 최적화함수\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# fitting\n",
    "history = model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}